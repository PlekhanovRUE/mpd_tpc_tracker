# Запуск поиска параметров

1) Установка необходимых библиотек

Если нет созданной виртуальной среды:
```shell
python3 -m venv venv
```

Если виртуальная среда уже создана, необходимо её активировать:
```shell
source venv/bin/activate
```

Установка библиотек:
```shell
pip install optuna
pip install optuna-integration
pip install botorch
```
2) Запуск скрипта:

```shell
python3 param_optimize/black-box-opt.py
```
Можно добавить опциональные параметры. Например:

```shell
python3 param_optimize/black-box-opt.py -logdir logs -n_trials 100 -method random -n_events 20
```

# Опциональные параметры скрипта:
* **-logdir** – директория для сохранения логов поиска гиперпараметров (<span style="color:blue">значение по умолчанию: log_params</span>, если такой папки нет, она будет создана в директории запуска скрипта);
* **-n_trials** – число "эпох", то есть количество вызовов целевой функции (<span style="color:blue">значение по умолчанию: None</span>, то есть перебор всех возможных значений. Если нет необходимости ограничивать поиск оптимальных значений количеством итераций, то не стоит указывать этот параметр);
* **-method** – выбор алгоритма для поиска гиперпараметров (см. ниже) (<span style="color:blue">значение по умолчанию: random</span>), доступные значения:
                + <span style="color:green">_random_</span> – случайный поиск (RandomSearch),
                + <span style="color:green">_tpe_</span> – древовидный парзеновский оценщик (англ. _Tree-structured Parzen Estimator, TPE_),
                + <span style="color:green">_cmaes_</span> – стратегия эволюции адаптации ковариационной матрицы (англ. _Covariance Matrix Adaptation Evolution Strategy, CMA-ES_),
                + <span style="color:green">_nsgaii_</span> – генетический алгоритм с недоминируемой сортировкой (англ. _Non-Dominated Sorting Genetic Algorithm, NSGA-II_), 
                + <span style="color:green">_qmc_</span> – квази Монте-Карло (англ. _Quasi-Monte Carlo, QMC_), 
                + <span style="color:green">_gp_</span> – байесовская оценка гауссовского процесса (англ. _Gaussian process-based Bayesian optimization, GP_),
                + <span style="color:green">_bayes_</span> – байесовская оптимизация (Bayesian Optimization).
* **-n_events** – число событий для трекера (<span style="color:blue">значение по умолчанию: 20</span>)

# Доступные алгоритмы оптимизации:

* **Random** – это метод, который выбирает случайные комбинации параметров из заданного пространства параметров;
* **TPE** – это метод, в котором 1) с помощью RandomSearch собирается некоторое число наблюдений; 2) задается два различных распределения гиперпараметров: первое при значениях целевой функции меньших, чем пороговое значение, второе - при значениях целевой функции больших, чем пороговое значение; 3) из второго распределения сэмплируются несколько значений-кандидатов; 4) из этих кандидатов находится тот, который с большей вероятностью окажется во второй группе (состоящей из лучших наблюдений); 5) этот кандидат используется для следующей итерации;
* **CMA-ES** – это метод, основанный на принципе биологической эволюции, а именно на повторяющемся взаимодействии вариаций (посредством рекомбинации и мутации) и отбора: в каждом поколении (итерации) новые особи (кандидаты на решение) генерируются путем вариации, обычно стохастическим образом, текущих родительских особей. На каждом поколении происходит перерасчёт матрицы ковариации, которая непосредственно влияет на размер и форму области, где будут генерироваться будущие потенциальные решения;
* **NSGA-II** – это метод, основная идея которого состоит в том, чтобы заставить совокупность возможных решений развиваться в направлении лучшего решения, чтобы решить задачу многокритериальной оптимизации, то есть находится набор оптимальных по Парето решений, так называемых недоминируемых решений. Недоминируемое решение – это решение, которое обеспечивает подходящий компромисс между всеми целями, не ухудшая при этом ни одну из них. Если какое-либо решение не доминирует над ним, решение x будет считаться недоминируемым и будет выбрано NSGA-II как одно из множества фронтов Парето;
* **QMC** – это метод, использующий последовательности с низким расхождением (также называемых квазислучайными последовательностями или субслучайными последовательностями) для достижения уменьшения дисперсии;
* **GP** – это байесовская оптимизация (BO), основанная на регрессии гауссовского процесса (GPR);
* **BAYES** - это итерационный метод, который на каждой итерации указывает наиболее вероятную точку, в которой целевая функция будет оптимальна. При этом выдаваемые вероятные точки включают две компоненты: 1) хорошая точка там, где согласно истории функция выдавала хорошие результаты на предыдущих вызовах (exploitation), 2) хорошая точка там, где высокая неопределенность, то есть неисследованные части пространства (exploration).

